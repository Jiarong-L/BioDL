{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4ac64-ce38-43df-80d9-d1ba4ab2acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, nn\n",
    "\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ce50e-27da-4ee5-a7c5-00e75f3f2b39",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8f5ef-2cf2-4acb-b4ea-4f23f21dd22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VFC_match = re.compile(r'[(]VFC(.*?)[)]', re.S)  # brick_match = re.compile(r'[\\[](.*?)[\\]]', re.S) \n",
    "\n",
    "VOCAB = [\"<unk>\"] + list('AFCUDNEQGHLIKOMPRSTVWYBZJX') + [\"<pad>\"]\n",
    "HEADERS = ['NotVF', 'VFC0001', 'VFC0346', 'VFC0083', 'VFC0235', 'VFC0258', 'VFC0272', 'VFC0315', 'VFC0325', 'VFC0086', 'VFC0204', 'VFC0271', 'VFC0301', 'VFC0251', 'VFC0282']\n",
    "BATCH_SIZE = 100\n",
    "EMBED_SIZE = 20\n",
    "MULTY_HEADER = 4\n",
    "assert EMBED_SIZE % 2 == 0\n",
    "assert MULTY_HEADER % 2 == 0\n",
    "assert EMBED_SIZE % MULTY_HEADER == 0\n",
    "\n",
    "def read_fasta(file):\n",
    "    empty_line_buffer = True\n",
    "    data_seq = []\n",
    "    data_header = []\n",
    "    with open(file,'r') as f:\n",
    "        while True:\n",
    "            line = f.readline().strip()      \n",
    "            if not line:\n",
    "                if empty_line_buffer:\n",
    "                    empty_line_buffer = False\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            empty_line_buffer = True\n",
    "            if '>' == line[0]:\n",
    "                data_header.append(line)\n",
    "                data_seq.append('')\n",
    "            else:\n",
    "                data_seq[-1] += line\n",
    "    return list(zip(data_seq,data_header))\n",
    "\n",
    "\n",
    "def tokenizer_embedder(seq):\n",
    "    return [VOCAB.index(amino) for amino in list(seq)]\n",
    "\n",
    "def header_parser(header):\n",
    "    if '>VF' == header[:3]:\n",
    "        return 'VFC' + re.findall(VFC_match, header)[0]\n",
    "    else:\n",
    "        return 'NotVF'\n",
    "\n",
    "def header_encoder(parsed_header):\n",
    "    return HEADERS.index(parsed_header)\n",
    "\n",
    "def collate_batch(data_batch, dtype=torch.float32, batch_first=False):\n",
    "    header_batch, seq_batch, seq_len = [], [], []\n",
    "    for seq,header in data_batch:\n",
    "        header_batch.append(header_encoder(header_parser(header)) )\n",
    "        seq_batch.append(torch.tensor(tokenizer_embedder(seq), dtype=dtype))   ## .unsqueeze(1) if no embedding layer (not suggest)\n",
    "        seq_len.append(len(seq))\n",
    "    header_batch = torch.tensor(header_batch)\n",
    "    seq_batch = torch.nn.utils.rnn.pad_sequence(seq_batch, padding_value=float(VOCAB.index('<pad>')), batch_first=batch_first) ## padded to equal\n",
    "    # if pack:\n",
    "        # seq_batch = nn.utils.rnn.pack_padded_sequence(seq_batch, seq_len, batch_first=batch_first, enforce_sorted=False) ## packed, for RNN/LSTM\n",
    "    return header_batch.to(device),seq_batch.to(device),torch.tensor(seq_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9117af-b5a1-458b-9485-afbf9d3c7b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0021828c-9c47-40e8-a3ad-8b89e6d97e88",
   "metadata": {},
   "source": [
    "## Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406bdacc-e5d6-4f18-8065-ab3e76006010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    lossSum = 0\n",
    "    model.train()                                    ### set training mode\n",
    "    for (header_batch, seq_batch, seq_len_batch) in dataloader:\n",
    "        pred = model(seq_batch, seq_len_batch)\n",
    "        # Compute prediction error\n",
    "        loss = loss_fn(pred,header_batch)\n",
    "        lossSum += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avgTrainingLoss = lossSum/len(dataloader)\n",
    "    return avgTrainingLoss\n",
    "\n",
    "\n",
    "def test_epoch(dataloader, model, loss_fn):\n",
    "    lossSum = 0\n",
    "    correctSum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (header_batch, seq_batch, seq_len_batch) in dataloader:\n",
    "            pred = model(seq_batch, seq_len_batch)\n",
    "            lossSum += loss_fn(pred,header_batch).item()\n",
    "            correctSum += (pred.argmax(1) == header_batch).type(torch.float).sum().item()\n",
    "    avgTestingLoss = lossSum/len(dataloader)             ## /num_batches\n",
    "    avgTestingAcc  = correctSum/len(dataloader.dataset)  ## /size\n",
    "    return avgTestingLoss,avgTestingAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17587a30-6a90-409c-acc4-fb1947bdab10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "146231d1-e671-4344-9617-cc50a02686b4",
   "metadata": {},
   "source": [
    "## Model: Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e859c-3097-4a23-883a-b0eeac4f3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Latest Suggestion: https://pytorch.org/tutorials/beginner/translation_transformer.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, batch_first):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.batch_first = batch_first\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        if self.batch_first:\n",
    "            pe = torch.zeros(1, max_len, d_model)\n",
    "            pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "            pe[0, :, 1::2] = torch.cos(position * div_term) ##  when batch_first=True: [batch, seq_len, dmodel]\n",
    "        else:\n",
    "            pe = torch.zeros(max_len, 1, d_model)\n",
    "            pe[:, 0, 0::2] = torch.sin(position * div_term) ##  when batch_first=False: [seq_len, batch, dmodel]\n",
    "            pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):   \n",
    "        if self.batch_first:\n",
    "            x = x[:,:self.pe.size(1)] + self.pe[:, : x.size(1), :]\n",
    "        else:\n",
    "            x = x[:self.pe.size(0)] + self.pe[:x.size(0)]   \n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, VOCAB_size, d_model, nhead, num_layers, out_class, batch_first):  ## Suggest: batch_first=True\n",
    "        super().__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.embedding = nn.Embedding(VOCAB_size, d_model)    ## turn each cell into emb_vector\n",
    "        self.pos_encoder = PositionalEncoding(d_model, 5000, batch_first)\n",
    "        encode_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=batch_first)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.TransformerEncoder = nn.TransformerEncoder(encode_layer, num_layers, encoder_norm)\n",
    "        self.linear = nn.Linear(d_model, out_class)\n",
    "        \n",
    "    def forward(self, input_batch, _):\n",
    "        x = self.embedding(input_batch)    ##  [seq_len <-> batch, dmodel]\n",
    "        x = self.pos_encoder(x)            ## |\n",
    "        x = self.TransformerEncoder(x)     ## |  \n",
    "        if self.batch_first:\n",
    "            x = self.linear(x[:,0,:])          \n",
    "        else:\n",
    "            x = self.linear(x[0,:,:])      ## output[batch, class_num]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd581c50-1d7c-42da-bf13-3bdef685822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_bfA = True\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(read_fasta('trainset.faa'), batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                       collate_fn = lambda x: collate_batch(x, dtype=torch.int, batch_first=if_bfA) )\n",
    "test_dl = torch.utils.data.DataLoader(read_fasta('testset.faa'), batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                      collate_fn = lambda x: collate_batch(x, dtype=torch.int, batch_first=if_bfA) )\n",
    "\n",
    "modelA = TransformerModel(len(VOCAB), EMBED_SIZE, MULTY_HEADER, 2, len(HEADERS), if_bfA )\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(modelA.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    avgTrainingLoss = train_epoch(train_dl, modelA, loss_fn, optimizer)\n",
    "    avgTestingLoss,avgTestingAcc = test_epoch(test_dl, modelA, loss_fn)\n",
    "    print(f'Epoch {t+1}----Testing Acc:: {avgTestingAcc:>7f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9aaaa1-81cc-472b-add3-7f61f47d2b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51b1d8-5115-44a3-9115-aa1b8f5277fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f23c3dda-4dad-496a-bae8-8b270af468a2",
   "metadata": {},
   "source": [
    "## Model: LSTM\n",
    "\n",
    "MANBA/LSTM -- NN -- category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f032e-9e35-4b73-805d-0bfd4893416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.cnblogs.com/BlueBlueSea/p/13723560.html\n",
    "class LSTM_Net(nn.Module):\n",
    "    def __init__(self, VOCAB_size, d_model, hidden_size, num_layers, out_class, batch_first):  ## Suggest: batch_first=False\n",
    "        super().__init__()\n",
    "        self.batch_first = batch_first\n",
    "        self.embedding = nn.Embedding(VOCAB_size, d_model)   \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=d_model, hidden_size=hidden_size, num_layers=num_layers, batch_first=batch_first, bidirectional=True\n",
    "        )                                         \n",
    "        self.linear = nn.Linear(hidden_size*2, out_class)\n",
    "    def forward(self, input_batch, seq_len_batch):\n",
    "        x = self.embedding(input_batch) \n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, seq_len_batch, batch_first=self.batch_first, enforce_sorted=False)\n",
    "        x,_ = self.lstm(x)\n",
    "        x,_ = nn.utils.rnn.pad_packed_sequence(x, padding_value=float(VOCAB.index('<pad>')), batch_first=self.batch_first)   ## [B<->SeqLen,2*hidden_size]\n",
    "        if self.batch_first:  \n",
    "            x = self.linear(x[:, -1, :])   ## [B!,SeqLen,2*hidden_size]\n",
    "        else:\n",
    "            x = self.linear(x[-1])         ## [SeqLen,B!,2*hidden_size]\n",
    "        return x\n",
    "\n",
    "\n",
    "## LSTM:  all, (hidden,cell) = self.lstm(x) ---> [2*num_layers <-> batch, 2*hidden_size]\n",
    "## h_final = x[:, -1, :] when batch_first=True   [B!,SeqLen,2*hidden_size]\n",
    "## h_final = x[-1]  when batch_first=False   [SeqLen,B!,2*hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10b8c3-450a-4787-a38f-c0f4c1e41bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_bfB = False\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(read_fasta('trainset.faa'), batch_size=BATCH_SIZE, shuffle=True, \n",
    "                                       collate_fn = lambda x: collate_batch(x, dtype=torch.int, batch_first=if_bfB) )\n",
    "test_dl = torch.utils.data.DataLoader(read_fasta('testset.faa'), batch_size=BATCH_SIZE, shuffle=False, \n",
    "                                      collate_fn = lambda x: collate_batch(x, dtype=torch.int, batch_first=if_bfB) )\n",
    "\n",
    "\n",
    "modelB = LSTM_Net(len(VOCAB), EMBED_SIZE, 33, 2, len(HEADERS), if_bfB )\n",
    "loss_fnB = nn.CrossEntropyLoss()\n",
    "optimizerB = torch.optim.SGD(modelB.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    avgTrainingLoss = train_epoch(train_dl, modelB, loss_fnB, optimizerB)\n",
    "    avgTestingLoss,avgTestingAcc = test_epoch(test_dl, modelB, loss_fnB)\n",
    "    print(f'Epoch {t+1}----Testing Acc:: {avgTestingAcc:>7f}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cd54b-1b1c-4a11-9166-82f1f45cfad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f5f00-2521-491b-b048-faf0be31493e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
