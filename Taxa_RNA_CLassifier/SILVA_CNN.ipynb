{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8800131,"sourceType":"datasetVersion","datasetId":5291866}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\n\ndevice = (\n    \"cuda:0\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:16:50.505257Z","iopub.execute_input":"2024-06-27T16:16:50.505554Z","iopub.status.idle":"2024-06-27T16:16:52.166729Z","shell.execute_reply.started":"2024-06-27T16:16:50.505526Z","shell.execute_reply":"2024-06-27T16:16:52.165764Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Using cuda:0 device\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load Dataset\n\nEmbed each bp with a 0/1 vector [x,x,x,x]\n\n**Tips: when BCEloss always =0.69 and acc=0.5, perhaps the model is too small, or please check dataloader**","metadata":{}},{"cell_type":"code","source":"DEG_dict = {   ## degenerative dict\n    'A':'A',\n    'U':'U',\n    'C':'C',\n    'G':'G',\n    'R':'AG',\n    'Y':'CT',\n    'M':'AC',\n    'K':'GT',\n    'S':'GC',\n    'W':'AT',\n    'H':'ATC',\n    'B':'GTC',\n    'V':'GAC',\n    'D':'GAT',\n    'N':'ATCG'\n    }\n\nORDER = list('AUCG')\n\ndef embed(nt):\n    emb = [0 for i in ORDER]\n    try:\n        for x in list(DEG_dict[nt]):\n            emb[ORDER.index(x)] = 1\n    except:\n        return [1 for i in ORDER]\n    return emb\n\n\ndef tokenize_embed(seq):\n    return [embed(nt) for nt in seq]","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:16:52.171817Z","iopub.execute_input":"2024-06-27T16:16:52.172114Z","iopub.status.idle":"2024-06-27T16:16:52.179522Z","shell.execute_reply.started":"2024-06-27T16:16:52.172064Z","shell.execute_reply":"2024-06-27T16:16:52.178647Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def read_fasta(file):\n    data_header = []\n    data_seq = []\n    with open(file,'r') as f:\n        while True:\n            lineH = f.readline().strip()      \n            lineS = f.readline().strip()   \n            if not lineS:\n                break\n            data_header.append(lineH)\n            data_seq.append(lineS)\n    return list(zip(data_header,data_seq)) \n\ndef header_parser_encoder(header):\n    if 'Archaea' in header.split(';')[0]:\n        return 0\n    else:\n        assert 'Bacteria' in header.split(';')[0]\n        return 1\n\n\ndef collate_batch(data_batch, dtype=torch.float32):\n    header_batch, seq_batch = [], []\n    for header,seq in data_batch:\n#         if header_parser_encoder(header) == 0:  \n#             seq = 'A'*1000\n#         else:                       ## For Fake Testing: in case the model is too small!!\n#             seq = 'U'*1000          ## and not capable for long & complex seqs\n        header_batch.append(header_parser_encoder(header))\n        seq_batch.append(torch.tensor(tokenize_embed(seq), dtype=dtype))\n    header_batch = torch.tensor(header_batch, dtype=dtype)\n    seq_batch = torch.nn.utils.rnn.pad_sequence(seq_batch, padding_value=float(0), batch_first=True) ## padded to equal\n    return header_batch.to(device),seq_batch.permute(0,2,1).to(device)    ## [batch, embd(channal), seqlen]    ","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:16:52.180866Z","iopub.execute_input":"2024-06-27T16:16:52.181199Z","iopub.status.idle":"2024-06-27T16:16:52.191025Z","shell.execute_reply.started":"2024-06-27T16:16:52.181169Z","shell.execute_reply":"2024-06-27T16:16:52.190129Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"KMER = 7   \nBATCH_SIZE = 512\nEMBED_SIZE = 120\nCLASS_NUM = 2\n\ntrain_dl = torch.utils.data.DataLoader(read_fasta('/kaggle/input/a000000/train.fa'), batch_size=BATCH_SIZE, shuffle=True, \n                                       collate_fn = lambda x: collate_batch(x, dtype=torch.float32) )","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:16:52.193037Z","iopub.execute_input":"2024-06-27T16:16:52.193317Z","iopub.status.idle":"2024-06-27T16:16:52.279102Z","shell.execute_reply.started":"2024-06-27T16:16:52.193294Z","shell.execute_reply":"2024-06-27T16:16:52.278384Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# for tt in train_dl:\n#     break\n\n# tt[1].size()\n# tt[0].size(), tt[1].size(), tt[0][0], tt[1][0]","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:16:52.280077Z","iopub.execute_input":"2024-06-27T16:16:52.280364Z","iopub.status.idle":"2024-06-27T16:16:52.283983Z","shell.execute_reply.started":"2024-06-27T16:16:52.280340Z","shell.execute_reply":"2024-06-27T16:16:52.283141Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Model\n\ninput: 16S seq\n\noutput: Archaea/Bacteria\n\n\nCNN --- LSTM --- Classifier\n\nTodo: Optimize the model, the current one seems not functioning\n\nTodo: Do not input strings, input kmer_abduance_list instead? (for small kmers, or the list could be too huge)","metadata":{}},{"cell_type":"code","source":"def train_epoch(dataloader, model, loss_fn, optimizer):\n    lossSum = 0\n    correctSum = 0\n    model.train()                                    ### set training mode\n    for (header_batch, seq_batch) in dataloader:\n        pred = model(seq_batch)\n        # Compute prediction error\n        loss = loss_fn(pred.squeeze(-1),header_batch)\n        lossSum += loss.item()\n        correctSum += (pred.squeeze(-1).ge(1/2) == header_batch).type(torch.float).sum().item()\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        print('>',end='')\n    avgTrainingLoss = lossSum/len(dataloader)\n    avgTrainingAcc  = correctSum/len(dataloader.dataset)\n    return avgTrainingLoss,avgTrainingAcc\n","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:16:52.284867Z","iopub.execute_input":"2024-06-27T16:16:52.285157Z","iopub.status.idle":"2024-06-27T16:16:52.294369Z","shell.execute_reply.started":"2024-06-27T16:16:52.285131Z","shell.execute_reply":"2024-06-27T16:16:52.293471Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class ClassifierBinary(nn.Module):\n    def __init__(self, sizeA): \n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(sizeA, sizeA//2),\n            nn.Dropout(),\n            nn.ReLU(),\n            \n            nn.Linear(sizeA//2, sizeA//4),\n            nn.Dropout(),\n            nn.ReLU(),\n            \n            nn.Linear(sizeA//4, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, x):\n        return self.mlp(x)\n\n\nclass CNN_RNN_Net(nn.Module):\n    def __init__(self, kmer):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv1d(in_channels = 4, out_channels = 32,  \n                      kernel_size = kmer, stride = 2),\n            nn.BatchNorm1d(32),## , affine=False\n            nn.MaxPool1d(kernel_size = kmer, stride = 2),\n            nn.ReLU(),\n            \n            \n            nn.Conv1d(in_channels = 32, out_channels = 64,  \n                      kernel_size = kmer, stride = 2),\n            nn.BatchNorm1d(64),\n            nn.MaxPool1d(kernel_size = 10, stride = 2),\n            nn.ReLU(),\n            \n            \n            nn.Conv1d(in_channels = 64, out_channels = 128,  \n                      kernel_size = kmer, stride = 2),\n            nn.BatchNorm1d(128),\n            nn.MaxPool1d(kernel_size = 10, stride = 2),\n            nn.ReLU(),\n        )\n        self.lstm = nn.LSTM(        \n            input_size=128, hidden_size=256, num_layers=1, batch_first=True, bidirectional=False\n        )\n        self.linearC = ClassifierBinary(256)\n        \n    def forward(self, input_batch):\n        x = self.cnn(input_batch)\n        x = x.permute(0,2,1)\n        _, (x, _) = self.lstm(x)\n        x = x.squeeze(0)\n        x = self.linearC(x)\n        return x\n\n\n\n# for (h,s) in train_dl:\n#     break\n\n# # s.size(),CNN_RNN_Net(7,2 )(s).size()\n# mm = CNN_RNN_Net(7).to(device)\n\n# mm(s).size()","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:16:52.295508Z","iopub.execute_input":"2024-06-27T16:16:52.295836Z","iopub.status.idle":"2024-06-27T16:16:52.308831Z","shell.execute_reply.started":"2024-06-27T16:16:52.295803Z","shell.execute_reply":"2024-06-27T16:16:52.307975Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"Model =  CNN_RNN_Net(KMER).to(device)\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.Adam(Model.parameters(), lr=1e-3)\n\n\nepochs = 5\nfor t in range(epochs):\n    avgTrainingLoss,avgTrainingAcc = train_epoch(train_dl, Model, loss_fn, optimizer)\n    print(f'Epoch {t+1}--Training loss:: {avgTrainingLoss:>7f}--Training Acc:: {avgTrainingAcc:>7f}') ","metadata":{"execution":{"iopub.status.busy":"2024-06-27T16:16:52.309835Z","iopub.execute_input":"2024-06-27T16:16:52.310150Z","iopub.status.idle":"2024-06-27T16:22:11.549734Z","shell.execute_reply.started":"2024-06-27T16:16:52.310120Z","shell.execute_reply":"2024-06-27T16:22:11.548722Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Epoch 1--Training loss:: 0.308338--Training Acc:: 0.841200\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Epoch 2--Training loss:: 0.008739--Training Acc:: 0.998150\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Epoch 3--Training loss:: 0.002549--Training Acc:: 0.999550\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Epoch 4--Training loss:: 0.000187--Training Acc:: 1.000000\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Epoch 5--Training loss:: 0.000103--Training Acc:: 1.000000\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}